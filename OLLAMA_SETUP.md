# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Ollama —Å MCP Mac Apps Server

## ‚úÖ –¢–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å

- ‚úÖ Ollama —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
- ‚úÖ –°–µ—Ä–≤–µ—Ä Ollama –∑–∞–ø—É—â–µ–Ω
- ‚úÖ –ú–æ–¥–µ–ª—å `llama3.2` –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (2.0 GB)
- ‚úÖ –ú–æ–¥–µ–ª—å `deepseek-r1:8b` –¥–æ—Å—Ç—É–ø–Ω–∞ (5.2 GB)

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### –í–∞—Ä–∏–∞–Ω—Ç 1: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ Claude Desktop

1. **–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ Claude Desktop** (–µ—Å–ª–∏ –µ—â–µ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω):
   - –°–∫–∞—á–∞–π—Ç–µ —Å https://claude.ai/download

2. **–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ MCP —Å–µ—Ä–≤–µ—Ä –¥–ª—è Ollama**:
   ```bash
   npx -y @modelcontextprotocol/create-server ollama-mcp
   ```
   
   –ò–ª–∏ –¥–æ–±–∞–≤—å—Ç–µ –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é Claude Desktop (`~/Library/Application Support/Claude/claude_desktop_config.json`):
   ```json
   {
     "mcpServers": {
       "ollama": {
         "command": "npx",
         "args": ["-y", "@modelcontextprotocol/server-ollama"]
       },
       "mac-apps": {
         "command": "node",
         "args": ["/Users/olegzaichkin/Documents/MCP/dist/index.js"]
       }
     }
   }
   ```

3. **–ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ Claude Desktop**

4. **–¢–µ–ø–µ—Ä—å Claude —Å–º–æ–∂–µ—Ç**:
   - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ Ollama
   - –£–ø—Ä–∞–≤–ª—è—Ç—å –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è–º–∏ –Ω–∞ Mac —á–µ—Ä–µ–∑ –≤–∞—à MCP —Å–µ—Ä–≤–µ—Ä

### –í–∞—Ä–∏–∞–Ω—Ç 2: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ API –Ω–∞–ø—Ä—è–º—É—é

Ollama –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç REST API –Ω–∞ `http://localhost:11434`. –í—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–≥–æ –Ω–∞–ø—Ä—è–º—É—é —Å –ª—é–±—ã–º –∫–ª–∏–µ–Ω—Ç–æ–º, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–º OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π API.

**–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ API:**
```bash
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "prompt": "–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?",
  "stream": false
}'
```

## üìù –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏

–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π:
```bash
ollama list
```

–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏:
```bash
# –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏
ollama pull llama3.1:8b       # –ë–æ–ª–µ–µ –º–æ—â–Ω–∞—è –≤–µ—Ä—Å–∏—è
ollama pull mistral:7b        # Mistral AI
ollama pull qwen2.5:7b        # Alibaba Qwen
ollama pull codellama:7b      # –î–ª—è –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è
ollama pull phi3              # –õ–µ–≥–∫–∞—è –º–æ–¥–µ–ª—å Microsoft
```

## üîß –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ Ollama

**–ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞:**
```bash
ollama serve
```

**–û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞:**
```bash
# –ù–∞–∂–º–∏—Ç–µ Ctrl+C –∏–ª–∏ –Ω–∞–π–¥–∏—Ç–µ –ø—Ä–æ—Ü–µ—Å—Å –∏ –∑–∞–≤–µ—Ä—à–∏—Ç–µ –µ–≥–æ
ps aux | grep ollama
kill <PID>
```

**–ê–≤—Ç–æ–∑–∞–ø—É—Å–∫ (macOS):**
Ollama –æ–±—ã—á–Ω–æ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —á–µ—Ä–µ–∑ LaunchAgent. –ï—Å–ª–∏ –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –≤ –∞–≤—Ç–æ–∑–∞–≥—Ä—É–∑–∫—É:
```bash
# –°–æ–∑–¥–∞—Ç—å LaunchAgent
cat > ~/Library/LaunchAgents/com.ollama.server.plist << EOF
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
    <key>Label</key>
    <string>com.ollama.server</string>
    <key>ProgramArguments</key>
    <array>
        <string>/opt/homebrew/bin/ollama</string>
        <string>serve</string>
    </array>
    <key>RunAtLoad</key>
    <true/>
    <key>KeepAlive</key>
    <true/>
</dict>
</plist>
EOF

# –ó–∞–≥—Ä—É–∑–∏—Ç—å –∞–≥–µ–Ω—Ç
launchctl load ~/Library/LaunchAgents/com.ollama.server.plist
```

## üéØ –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### –¢–µ—Å—Ç –º–æ–¥–µ–ª–∏ –Ω–∞–ø—Ä—è–º—É—é:
```bash
ollama run llama3.2 "–†–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ MCP –ø—Ä–æ—Ç–æ–∫–æ–ª"
```

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ API —Å curl:
```bash
# –ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "prompt": "–ß—Ç–æ —Ç–∞–∫–æ–µ Model Context Protocol?",
  "stream": false
}'

# –° —Å—Ç—Ä–∏–º–∏–Ω–≥–æ–º
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "prompt": "–ü—Ä–∏–≤–µ—Ç!",
  "stream": true
}'
```

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å Python:
```python
import requests
import json

response = requests.post(
    'http://localhost:11434/api/generate',
    json={
        'model': 'llama3.2',
        'prompt': '–û—Ç–∫—Ä–æ–π Safari',
        'stream': False
    }
)

print(response.json()['response'])
```

## üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç—ã

–ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ —Å–µ—Ä–≤–µ—Ä —Ä–∞–±–æ—Ç–∞–µ—Ç:
```bash
curl http://localhost:11434/api/tags
```

–î–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON.

## üí° –°–æ–≤–µ—Ç—ã

1. **–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å**: –ú–æ–¥–µ–ª—å `llama3.2` (2GB) —Ä–∞–±–æ—Ç–∞–µ—Ç –±—ã—Å—Ç—Ä–æ, –Ω–æ –º–µ–Ω–µ–µ –º–æ—â–Ω–∞—è. –î–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ `llama3.1:8b` –∏–ª–∏ `deepseek-r1:8b`.

2. **–ü–∞–º—è—Ç—å**: –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —É –≤–∞—Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ RAM. –ú–æ–¥–µ–ª–∏ —Ç—Ä–µ–±—É—é—Ç:
   - `llama3.2`: ~2-4 GB RAM
   - `llama3.1:8b`: ~8-10 GB RAM
   - `deepseek-r1:8b`: ~10-12 GB RAM

3. **–°–∫–æ—Ä–æ—Å—Ç—å**: –ù–∞ Mac —Å Apple Silicon (M1/M2/M3) –º–æ–¥–µ–ª–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –±—ã—Å—Ç—Ä–µ–µ –±–ª–∞–≥–æ–¥–∞—Ä—è –Ω–µ–π—Ä–æ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—É.

4. **–ü—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å**: –í—Å–µ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ª–æ–∫–∞–ª—å–Ω–æ, –¥–∞–Ω–Ω—ã–µ –Ω–∏–∫—É–¥–∞ –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è—é—Ç—Å—è.

## üìö –ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏

- [Ollama –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è](https://ollama.ai/docs)
- [–î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏](https://ollama.ai/library)
- [Ollama GitHub](https://github.com/ollama/ollama)

